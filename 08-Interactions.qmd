# Non-linear terms

```{r}
# install.packages(c("tidyverse"))
library(tidyverse)
```

## Simpson's paradox

The data set `gdp2019` contains the GDP per capita in current prices for 142 countries in 2019. In addition, the variable `spending` shows the government spending in these countries as a share (in percent) of GDP.

-   `gdppc` (numeric): [GDP per capita](https://www.imf.org/external/datamapper/NGDPDPC@WEO/OEMDC/ADVEC/WEOWORLD) in USD (current prices)
-   `spending` (numeric): [Government spending as a share of GDP](https://www.imf.org/external/datamapper/exp@FPP/USA/FRA/JPN/GBR/SWE/ESP/ITA/ZAF/IND).

The data set was constructed using data from the IFM. You can find more information about the two variables by following the links above.

We want to explore the relationship between `gdppc` and `spending`.

```{r}
gdp2019 <- read_csv(
  "https://raw.githubusercontent.com/feb-sofia/econometrics-2023/main/data/gdpgov2019.csv"
  ) %>%
  filter(!is.na(spending))
```

1.  Create a scatterplot for the two variables and add the estimated regression line for the model

$$
\text{gdppc}_i = \beta_0 + \beta_1 \text{spending}_i + e_i, \quad e_i \sim N(0, \sigma^2)
$$

```{r, fig.cap="GDP per capita and government spending as percent of GDP."}
gdp2019 %>%
  ggplot(aes(x = spending, y = gdppc)) + 
  geom_point() +
  geom_smooth(method = "lm") + 
  labs(
    y = "GDP per capita",
    x = "Gov. spending"
  )
```

2.  Estimate the linear model

```{r}
fit1 <- lm(gdppc ~ 1 + spending, data = gdp2019)
summary(fit1)
```

2.  Create a new variable called `gdppc_gr` that has five categories:

-   Low: if $\text{gdppc} \leq 1025$
-   Medium-low: if $1025 < \text{gdppc} \leq 3995$
-   Medium-high: if $3995 < \text{gdppc} \leq 12375$
-   High: if $12375 < \text{gdppc} \leq 30000$
-   Very high: if $\text{gdppc} > 30000$

```{r}
gdp2019 <- gdp2019 %>%
  mutate(
    gdppc_gr = cut(
      gdppc,
      breaks = c(0, 1025, 3995, 12375, 30000, Inf),
      labels = c("Low", "Medium-low", "Medium-high", "High", "Very high"))
  )
```

3.  Count how many countries fall into each category (`gdppc_gr`)

```{r}
table(gdp2019$gdppc_gr)
```

4.  Estimate the following linear model and interpret the estimated coefficients

$$
\text{gdppc}_i = \beta_0 + \beta_{\text{gdppc_gr}[i]} + e_i, e_i \sim N(0, \sigma^2)
$$

```{r}
# gdp2019 %>%
#   group_by(gdppc_gr) %>%
#   summarise(
#     ...
#   )
```

```{r}
fit2 <- lm(gdppc ~ 1 + gdppc_gr, data = gdp2019)
summary(fit2)
```

5.  Estimate the following linear model and interpret the estimated coefficients

$$
\text{gdppc}_i = \beta_0 + \beta_{\text{gdppc_gr}[i]} + \beta_1 \text{spending}_i + e_i, e_i \sim N(0, \sigma^2)
$$

```{r}
fit3 <- update(fit2, . ~ . + spending)
summary(fit3)
```

6.  Plot the estimated regression lines

```{r}
gdp2019 %>%
  ggplot(aes(x = spending, y = gdppc, color = gdppc_gr)) + 
  geom_point() + 
  geom_abline(
    intercept = c(-144.39, 1308.32, 5835.94, 18460.21, 51920.69),
    slope = 44.80,
    alpha = 0.5
  )
```

Very often we want test the hypothesis that a certain subset of the coefficients are simultaneously equal to zero. This is called a joint hypothesis.

In our example we could test the hypothesis that the coefficients of `Med Low`, `Med High`, `High` and `Very High` are all equal to zero. This is equivalent to testing if the average of `gdppc` is the same in all groups.

The test is called an F-test and is based on the ratio of the sum of squared errors of the restricted model and the sum of squared errors of the unrestricted model.

The restricted model is the one where the coefficients of `Med Low`, `Med High`, `High` and `Very High` are all equal to zero.

The unrestricted model is the one where all coefficients are estimated.

The F-test is based on the following statistic:

$$
F = \frac{(RSS_{\text{restricted}} - RSS_{\text{unrestricted}}) / (p_{\text{restricted}} - p_{\text{unrestricted}})}{RSS_{\text{unrestricted}} / (n - p_{\text{unrestricted}})}
$$

where $p$ is the number of coefficients in the model and $n$ is the number of observations.

```{r}
fit2
```

```{r}
if (!require("car")) install.packages("car")

library(car)
linearHypothesis(fit2, c("gdppc_grMedium-low - gdppc_grMedium-high = 0"))
```

## Interaction effect

```{r}
# Load the data
stu <- read.csv("https://raw.githubusercontent.com/febse/data/main/econ/drinking.csv") %>%
  mutate(
    sex = ifelse(male == 1, "Male", "Female")
  )

# Print the first few rows
stu %>% head()
```

```{r}
stu %>%
  ggplot(aes(x=gpa, y=drink)) +
    geom_point() +
    geom_smooth(method=lm)
```

```{r}
m1 <- lm(drink ~ gpa, data = stu)
m1 %>% summary()
```

```{r}
m2 <- lm(drink ~ sex + gpa, data = stu)
m2 %>% summary()
```

```{r}
m3 <- lm(drink ~ sex * gpa, data = stu)
m3 %>% summary()
```


## Polynomial regression

In the following we will simulate 100 observations from the following model

$$
y_i = 2 + x_i + x^2 + e_i, e_i \sim N(0, 1)
$$

```{r}
## Create the simulated sample
sim_n <- 100

poly_dt <- tibble(
  x = runif(n = sim_n, min = -4, max = 3),
  y = 2 +  x +  x^2 + rnorm(n = sim_n, mean = 0, sd = 1)
)
```

1.  Plot `x` and `y` (scatterplot)

```{r}
poly_dt %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point()
```

2.  Estimate a linear model for `x` and `y`

$$
y_i = \beta_0 + \beta_1 x_i, e_i \sim N(0, \sigma^2)
$$

```{r}
fit1 <- lm(y ~ x, data = poly_dt)
summary(fit1)
```

```{r}
fit1_pred <- predict(
  fit1, 
  interval = "confidence"
  ) %>%
  as_tibble() %>%
  bind_cols(poly_dt) %>%
  mutate(
    res = y - fit
  )
```

Plot the fitted values (model predictions) against the residuals.

```{r}
fit1_pred %>%
  ggplot(aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0, alpha = 0.5, lty = 2) + 
  labs(
    x = "Predicted",
    y = "Residual"
  )
```

3.  Add a quadratic term to the linear model and estimate it

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + e_i, e_i \sim N(0, \sigma^2)
$$

```{r}
fit2 <- update(fit1, . ~ . + I(x^2))
summary(fit2)
```

```{r}
fit2_pred <- predict(fit2, interval = "confidence") %>%
  as_tibble() %>%
  bind_cols(poly_dt) %>%
  mutate(
    res = y - fit
  )
```

```{r}
fit2_pred %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(
    aes(y = fit),
    color = "steelblue2"
  ) + 
  geom_ribbon(
    aes(ymin = lwr, ymax = upr), 
    alpha = 0.2, 
    fill = "steelblue2"
  )
```

Residuals vs fitted plot

```{r}
fit2_pred %>%
  ggplot(aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0, alpha = 0.5, lty = 2) + 
  labs(
    x = "Predicted",
    y = "Residual"
  )
```

4.  Add the third and fourth degree polynomial terms to the model and estimate it

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i + \beta_4 x^4_i + e_i, e_i \sim N(0, \sigma^2)
$$

```{r}
fit3 <- update(fit2, . ~ . + I(x ^ 3) + I(x ^ 4))
summary(fit3)
```

```{r}
fit3_pred <- predict(fit3, interval = "confidence") %>%
  as_tibble() %>%
  bind_cols(poly_dt) %>%
  mutate(
    res = y - fit
  )
```

```{r}
fit3_pred %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(
    aes(y = fit),
    color = "steelblue2"
  ) + 
  geom_ribbon(
    aes(ymin = lwr, ymax = upr), 
    alpha = 0.2, 
    fill = "steelblue2"
  )
```

```{r}
fit3_pred %>%
  ggplot(aes(x = fit, y = res)) + 
  geom_point() + 
  geom_hline(yintercept = 0, alpha = 0.5, lty = 2) + 
  labs(
    x = "Predicted",
    y = "Residual"
  )
```

## Model choice

1.  $R^2$

$$
R^2 = 1 - \frac{RSS}{TSS} \\
RSS = \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 \\
TSS = \sum_{i = 1}^{n}(y_i - \bar{y})^2
$$

Compute $R^2$ for the three models. Which model fits the data best according to it?

```{r}
summary(fit1)$r.squared
summary(fit2)$r.squared
summary(fit3)$r.squared
```

Note: don't use $R^2$ in models without a constant

```{r}
sim_noconst <- tibble(
  y = 2 + rnorm(n = 100, mean = 0, sd = 1),
  x = runif(n = 100)
)

sim_noconst %>%
  ggplot(aes(x = x, y = y)) + 
  geom_point()
```

```{r}
summary(lm(y ~ 1 + x, data = sim_noconst))
summary(lm(y ~ 0 + x, data = sim_noconst))
```

2.  Adjusted $R^2$

$$
R^2_{ADJ} = 1 - \frac{n - 1}{n - p - 1} \frac{RSS}{TSS}
$$

3.  Information Criteria

Compute the Akaike information criterion (AIC) for the three models and choose the best one.

$$
\text{AIC} = n \log\left(\frac{RSS}{n}\right) + 2p + n + n \log(2\pi).
$$

```{r}
AIC(fit1)
AIC(fit2)
AIC(fit3)
```

4. Validation

```{r}
smpl <- sample(1:100, 20)
poly_dt_estim <- poly_dt[smpl, ]
poly_dt_test <- poly_dt[-smpl, ]

fit2_estim <- lm(y ~ poly(x, 4), data = poly_dt_estim)
fit5_test <- lm(y ~ poly(x, 4), data = poly_dt_test)

pred2 <- predict(fit2_estim, newdata = poly_dt_estim)
pred5 <- predict(fit2_estim, newdata = poly_dt_estim)

# TODO: calculate the MSE in the test data

```

